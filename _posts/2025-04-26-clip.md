---
layout: distill
title: Understanding CLIP embeddings
description: A quick oveview of OpenAI's CLIP embeddings
tags: CLIP
giscus_comments: false
date: 2025-04-26
featured: false

authors:
  - name: Manik Bhandari
    affiliations:
      name: Personal

bibliography: clip.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
# toc:
#   - name: Equations
#     # if a section has subsections, you can add them as follows:
#     # subsections:
#     #   - name: Example Child Subsection 1
#     #   - name: Example Child Subsection 2
#   - name: Citations
#   - name: Footnotes
#   - name: Code Blocks
#   - name: Interactive Plots
#   - name: Layouts
#   - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Summary

- Paper from OpenAI published in ICML 2021 <d-cite key=radford2021learning></d-cite>

- Using symmetric contrastive loss <d-footnote>CLIP stands for Contrastive Language-Image Pretraining</d-footnote> to train an embedding model over a large datset of 400M image-text pairs yields good embeddings.

- Simplified the ConVIRT <d-cite key="zhang2021contrastive"></d-cite> architecture.
- Created a new dataset of 400M datapoints.

---

## Approach

<div class="caption">
    CLIP training algorithm pseudocode.
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blog/clip/clip-algorithm.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="caption">
    CLIP training and inference diagram.
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blog/clip/clip-diagram.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

---

## Proposed Dataset

The paper doesn't seem to give any details on how this dataset was constructed. It simply mentions that they "searched" <d-footnote>What does it mean to "search", search how?</d-footnote> for image-text pairs where text includes an item from one of 500K list of items. Kept upto 20K image-text pairs per "query".<d-footnote>What is a query?</d-footnote>

> The base query list <d-footnote>this is the 500K terms list?</d-footnote> is all words occurring at least 100 times in the English version of Wikipedia. This is augmented with bi-grams with high pointwise mutual information as well as the names of all Wikipedia articles above a certain search volume. Finally all WordNet synsets not already in the query list are added
