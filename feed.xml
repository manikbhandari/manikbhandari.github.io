<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml"/><link href="/" rel="alternate" type="text/html"/><updated>2025-06-05T15:00:36+00:00</updated><id>/feed.xml</id><title type="html">blank</title><subtitle>Manik Bhandari&apos;s website. </subtitle><entry><title type="html">Understanding CLIP embeddings</title><link href="/blog/2025/clip/" rel="alternate" type="text/html" title="Understanding CLIP embeddings"/><published>2025-04-26T00:00:00+00:00</published><updated>2025-04-26T00:00:00+00:00</updated><id>/blog/2025/clip</id><content type="html" xml:base="/blog/2025/clip/"><![CDATA[<h2 id="summary">Summary</h2> <ul> <li> <p>Paper from OpenAI published in ICML 2021 <d-cite key="radford2021learning"></d-cite></p> </li> <li> <p>Using symmetric contrastive loss <d-footnote>CLIP stands for Contrastive Language-Image Pretraining</d-footnote> to train an embedding model over a large datset of 400M image-text pairs yields good embeddings.</p> </li> <li>Simplified the ConVIRT <d-cite key="zhang2021contrastive"></d-cite> architecture.</li> <li>Created a new dataset of 400M datapoints.</li> </ul> <hr/> <h2 id="approach">Approach</h2> <div class="caption"> CLIP training algorithm pseudocode. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/clip/clip-algorithm.png" sizes="95vw"/> <img src="/assets/img/blog/clip/clip-algorithm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CLIP training and inference diagram. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/clip/clip-diagram.png" sizes="95vw"/> <img src="/assets/img/blog/clip/clip-diagram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="proposed-dataset">Proposed Dataset</h2> <p>The paper doesn’t seem to give any details on how this dataset was constructed. It simply mentions that they “searched” <d-footnote>What does it mean to "search", search how?</d-footnote> for image-text pairs where text includes an item from one of 500K list of items. Kept upto 20K image-text pairs per “query”.<d-footnote>What is a query?</d-footnote></p> <blockquote> <p>The base query list <d-footnote>this is the 500K terms list?</d-footnote> is all words occurring at least 100 times in the English version of Wikipedia. This is augmented with bi-grams with high pointwise mutual information as well as the names of all Wikipedia articles above a certain search volume. Finally all WordNet synsets not already in the query list are added</p> </blockquote>]]></content><author><name>Manik Bhandari</name></author><category term="CLIP"/><summary type="html"><![CDATA[A quick oveview of OpenAI's CLIP embeddings]]></summary></entry></feed>